<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: mesos | As a SW/Ops/DB Engineer]]></title>
  <link href="http://tech.riywo.com/blog/categories/mesos/atom.xml" rel="self"/>
  <link href="http://tech.riywo.com/"/>
  <updated>2014-12-26T01:21:17+09:00</updated>
  <id>http://tech.riywo.com/</id>
  <author>
    <name><![CDATA[riywo]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Terraform module for Mesos + Ceph cluster and Packer template]]></title>
    <link href="http://tech.riywo.com/blog/2014/12/25/terraform-module-for-mesos-plus-ceph-cluster-and-packer-template/"/>
    <updated>2014-12-25T23:13:16+09:00</updated>
    <id>http://tech.riywo.com/blog/2014/12/25/terraform-module-for-mesos-plus-ceph-cluster-and-packer-template</id>
    <content type="html"><![CDATA[<p><a href="https://github.com/riywo/mesos-ceph">riywo/mesos-ceph</a></p>

<p>I&rsquo;ve just tried to use <a href="https://www.terraform.io">Terraform</a> and <a href="https://www.packer.io">Packer</a> to create a <a href="http://mesos.apache.org">Mesos</a> + <a href="http://ceph.com">Ceph</a> cluster in <a href="http://aws.amazon.com">AWS</a>. Yes, I know <a href="http://mesosphere.com">Mesosphere</a> applications supporting deployment of Mesos cluster in some IaaS (see <a href="http://mesosphere.com/docs/getting-started">Getting Started</a>), but I&rsquo;d like to understand what&rsquo;s going on there. So, I did it by Terraform and Packer. I&rsquo;m gonna explain a little bit more about this.</p>

<p>Through this work, I&rsquo;ve learned a lot of things. I will write something below too.</p>

<!-- more -->


<h2>What is this?</h2>

<p>This repository includes Terraform module and Packer template. Terraform module will manage a VPC subnet, igw, sg, etc. and instances below:</p>

<ul>
<li>admin

<ul>
<li>ssh gateway</li>
<li>run <code>ceph-deploy</code></li>
</ul>
</li>
<li>master1, master2, master3

<ul>
<li>mesos master</li>
<li>marathon</li>
<li>ceph mon</li>
<li>ceph mds</li>
<li>mount cephfs</li>
</ul>
</li>
<li>slaves (default 3)

<ul>
<li>mesos slave</li>
<li>ceph osd with EBS</li>
<li>mount cephfs</li>
</ul>
</li>
</ul>


<p>Why not only Mesos, did I manage Ceph cluster? In Mesos, I&rsquo;d like to use a shared file system because:</p>

<ul>
<li>Share executer files, etc.</li>
<li>Control cluster software on Mesos</li>
<li>Save uploaded files to Mesos tasks (ex. image files to blog apps)</li>
</ul>


<p>Or just for fun :)</p>

<h2>How to use?</h2>

<p>Very simple, make your own terraform config like below:</p>

<pre><code>provider "aws" {}

resource "aws_vpc" "default" {
        cidr_block           = "10.0.0.0/16"
        enable_dns_support   = true
        enable_dns_hostnames = true
}

module "mesos_ceph" {
        source                   = "github.com/riywo/mesos-ceph/terraform"
        vpc_id                   = "${aws_vpc.default.id}"
        key_name                 = "${var.key_name}"
        key_path                 = "${var.key_path}"
        subnet_availability_zone = "us-east-1e"
        subnet_cidr_block        = "10.0.1.0/24"
        master1_ip               = "10.0.1.11"
        master2_ip               = "10.0.1.12"
        master3_ip               = "10.0.1.13"
}
</code></pre>

<p>Note: You can use environment variables for AWS configuration and <code>terraform.tfvars</code> file for variables.</p>

<pre><code>export AWS_REGION=us-east-1
export AWS_ACCESS_KEY=AAAAAAAAAA
export AWS_SECRET_KEY=000000000000000000

$ cat terraform.tfvars
key_name = "your key name"
key_path = "/path/to/private_pem_file"
</code></pre>

<p>First, you should download the module using <code>terraform get</code></p>

<pre><code>$ terraform get
Get: git::https://github.com/riywo/mesos-ceph.git
</code></pre>

<p>Then, you can check <code>terraform plan</code>. To show resources in module, you have to provide <code>-module-depth</code> option.</p>

<pre><code>$ terraform plan -module-depth -1
Refreshing Terraform state prior to plan...


The Terraform execution plan has been generated and is shown below.
Resources are shown in alphabetical order for quick scanning. Green resources
will be created (or destroyed and then created if an existing resource
exists), yellow resources are being changed in-place, and red resources
will be destroyed.

Note: You didn't specify an "-out" parameter to save this plan, so when
"apply" is called, Terraform can't guarantee this is what will execute.

+ aws_vpc.default
    cidr_block:           "" =&gt; "10.0.0.0/16"
    enable_dns_hostnames: "" =&gt; "1"
    enable_dns_support:   "" =&gt; "1"
    main_route_table_id:  "" =&gt; "&lt;computed&gt;"

+ module.mesos.aws_instance.admin
    ami:               "" =&gt; "ami-06ef816e"
    availability_zone: "" =&gt; "&lt;computed&gt;"
    instance_type:     "" =&gt; "t2.micro"
    key_name:          "" =&gt; "your key name"
    private_dns:       "" =&gt; "&lt;computed&gt;"
    private_ip:        "" =&gt; "&lt;computed&gt;"
    public_dns:        "" =&gt; "&lt;computed&gt;"
    public_ip:         "" =&gt; "&lt;computed&gt;"
    security_groups.#: "" =&gt; "&lt;computed&gt;"
    subnet_id:         "" =&gt; "${aws_subnet.public.id}"
    tenancy:           "" =&gt; "&lt;computed&gt;"

+ module.mesos.aws_instance.master1
    ami:               "" =&gt; "ami-06ef816e"
    availability_zone: "" =&gt; "&lt;computed&gt;"
    instance_type:     "" =&gt; "t2.micro"
    key_name:          "" =&gt; "your key name"
    private_dns:       "" =&gt; "&lt;computed&gt;"
    private_ip:        "" =&gt; "10.0.1.11"
    public_dns:        "" =&gt; "&lt;computed&gt;"
    public_ip:         "" =&gt; "&lt;computed&gt;"
    security_groups.#: "" =&gt; "&lt;computed&gt;"
    subnet_id:         "" =&gt; "${aws_subnet.public.id}"
    tenancy:           "" =&gt; "&lt;computed&gt;"

+ module.mesos.aws_instance.master2
    ami:               "" =&gt; "ami-06ef816e"
    availability_zone: "" =&gt; "&lt;computed&gt;"
    instance_type:     "" =&gt; "t2.micro"
    key_name:          "" =&gt; "your key name"
    private_dns:       "" =&gt; "&lt;computed&gt;"
    private_ip:        "" =&gt; "10.0.1.12"
    public_dns:        "" =&gt; "&lt;computed&gt;"
    public_ip:         "" =&gt; "&lt;computed&gt;"
    security_groups.#: "" =&gt; "&lt;computed&gt;"
    subnet_id:         "" =&gt; "${aws_subnet.public.id}"
    tenancy:           "" =&gt; "&lt;computed&gt;"

+ module.mesos.aws_instance.master3
    ami:               "" =&gt; "ami-06ef816e"
    availability_zone: "" =&gt; "&lt;computed&gt;"
    instance_type:     "" =&gt; "t2.micro"
    key_name:          "" =&gt; "your key name"
    private_dns:       "" =&gt; "&lt;computed&gt;"
    private_ip:        "" =&gt; "10.0.1.13"
    public_dns:        "" =&gt; "&lt;computed&gt;"
    public_ip:         "" =&gt; "&lt;computed&gt;"
    security_groups.#: "" =&gt; "&lt;computed&gt;"
    subnet_id:         "" =&gt; "${aws_subnet.public.id}"
    tenancy:           "" =&gt; "&lt;computed&gt;"

+ module.mesos.aws_instance.slaves.0
    ami:                                  "" =&gt; "ami-06ef816e"
    availability_zone:                    "" =&gt; "&lt;computed&gt;"
    block_device.#:                       "" =&gt; "1"
    block_device.0.delete_on_termination: "" =&gt; "1"
    block_device.0.device_name:           "" =&gt; "/dev/sdb"
    block_device.0.encrypted:             "" =&gt; ""
    block_device.0.snapshot_id:           "" =&gt; ""
    block_device.0.virtual_name:          "" =&gt; ""
    block_device.0.volume_size:           "" =&gt; "30"
    block_device.0.volume_type:           "" =&gt; ""
    instance_type:                        "" =&gt; "t2.micro"
    key_name:                             "" =&gt; "your key name"
    private_dns:                          "" =&gt; "&lt;computed&gt;"
    private_ip:                           "" =&gt; "&lt;computed&gt;"
    public_dns:                           "" =&gt; "&lt;computed&gt;"
    public_ip:                            "" =&gt; "&lt;computed&gt;"
    security_groups.#:                    "" =&gt; "&lt;computed&gt;"
    subnet_id:                            "" =&gt; "${aws_subnet.public.id}"
    tenancy:                              "" =&gt; "&lt;computed&gt;"

+ module.mesos.aws_instance.slaves.1
    ami:                                  "" =&gt; "ami-06ef816e"
    availability_zone:                    "" =&gt; "&lt;computed&gt;"
    block_device.#:                       "" =&gt; "1"
    block_device.0.delete_on_termination: "" =&gt; "1"
    block_device.0.device_name:           "" =&gt; "/dev/sdb"
    block_device.0.encrypted:             "" =&gt; ""
    block_device.0.snapshot_id:           "" =&gt; ""
    block_device.0.virtual_name:          "" =&gt; ""
    block_device.0.volume_size:           "" =&gt; "30"
    block_device.0.volume_type:           "" =&gt; ""
    instance_type:                        "" =&gt; "t2.micro"
    key_name:                             "" =&gt; "your key name"
    private_dns:                          "" =&gt; "&lt;computed&gt;"
    private_ip:                           "" =&gt; "&lt;computed&gt;"
    public_dns:                           "" =&gt; "&lt;computed&gt;"
    public_ip:                            "" =&gt; "&lt;computed&gt;"
    security_groups.#:                    "" =&gt; "&lt;computed&gt;"
    subnet_id:                            "" =&gt; "${aws_subnet.public.id}"
    tenancy:                              "" =&gt; "&lt;computed&gt;"

+ module.mesos.aws_instance.slaves.2
    ami:                                  "" =&gt; "ami-06ef816e"
    availability_zone:                    "" =&gt; "&lt;computed&gt;"
    block_device.#:                       "" =&gt; "1"
    block_device.0.delete_on_termination: "" =&gt; "1"
    block_device.0.device_name:           "" =&gt; "/dev/sdb"
    block_device.0.encrypted:             "" =&gt; ""
    block_device.0.snapshot_id:           "" =&gt; ""
    block_device.0.virtual_name:          "" =&gt; ""
    block_device.0.volume_size:           "" =&gt; "30"
    block_device.0.volume_type:           "" =&gt; ""
    instance_type:                        "" =&gt; "t2.micro"
    key_name:                             "" =&gt; "your key name"
    private_dns:                          "" =&gt; "&lt;computed&gt;"
    private_ip:                           "" =&gt; "&lt;computed&gt;"
    public_dns:                           "" =&gt; "&lt;computed&gt;"
    public_ip:                            "" =&gt; "&lt;computed&gt;"
    security_groups.#:                    "" =&gt; "&lt;computed&gt;"
    subnet_id:                            "" =&gt; "${aws_subnet.public.id}"
    tenancy:                              "" =&gt; "&lt;computed&gt;"

+ module.mesos.aws_internet_gateway.public
    vpc_id: "" =&gt; "${var.vpc_id}"

+ module.mesos.aws_route_table.public
    route.#:             "" =&gt; "1"
    route.0.cidr_block:  "" =&gt; "0.0.0.0/0"
    route.0.gateway_id:  "" =&gt; "${aws_internet_gateway.public.id}"
    route.0.instance_id: "" =&gt; ""
    vpc_id:              "" =&gt; "${var.vpc_id}"

+ module.mesos.aws_route_table_association.public
    route_table_id: "" =&gt; "${aws_route_table.public.id}"
    subnet_id:      "" =&gt; "${aws_subnet.public.id}"

+ module.mesos.aws_security_group.maintenance
    description:                 "" =&gt; "maintenance"
    ingress.#:                   "" =&gt; "1"
    ingress.0.cidr_blocks.#:     "" =&gt; "1"
    ingress.0.cidr_blocks.0:     "" =&gt; "0.0.0.0/0"
    ingress.0.from_port:         "" =&gt; "22"
    ingress.0.protocol:          "" =&gt; "tcp"
    ingress.0.security_groups.#: "" =&gt; "0"
    ingress.0.self:              "" =&gt; "0"
    ingress.0.to_port:           "" =&gt; "22"
    name:                        "" =&gt; "maintenance"
    owner_id:                    "" =&gt; "&lt;computed&gt;"
    vpc_id:                      "" =&gt; "${var.vpc_id}"

+ module.mesos.aws_security_group.master
    description:                 "" =&gt; "master"
    ingress.#:                   "" =&gt; "2"
    ingress.0.cidr_blocks.#:     "" =&gt; "1"
    ingress.0.cidr_blocks.0:     "" =&gt; "0.0.0.0/0"
    ingress.0.from_port:         "" =&gt; "8080"
    ingress.0.protocol:          "" =&gt; "tcp"
    ingress.0.security_groups.#: "" =&gt; "0"
    ingress.0.self:              "" =&gt; "0"
    ingress.0.to_port:           "" =&gt; "8080"
    ingress.1.cidr_blocks.#:     "" =&gt; "1"
    ingress.1.cidr_blocks.0:     "" =&gt; "0.0.0.0/0"
    ingress.1.from_port:         "" =&gt; "5050"
    ingress.1.protocol:          "" =&gt; "tcp"
    ingress.1.security_groups.#: "" =&gt; "0"
    ingress.1.self:              "" =&gt; "0"
    ingress.1.to_port:           "" =&gt; "5050"
    name:                        "" =&gt; "master"
    owner_id:                    "" =&gt; "&lt;computed&gt;"
    vpc_id:                      "" =&gt; "${var.vpc_id}"

+ module.mesos.aws_security_group.private
    description:                 "" =&gt; "private"
    ingress.#:                   "" =&gt; "3"
    ingress.0.cidr_blocks.#:     "" =&gt; "0"
    ingress.0.from_port:         "" =&gt; "0"
    ingress.0.protocol:          "" =&gt; "udp"
    ingress.0.security_groups.#: "" =&gt; "0"
    ingress.0.self:              "" =&gt; "1"
    ingress.0.to_port:           "" =&gt; "65535"
    ingress.1.cidr_blocks.#:     "" =&gt; "0"
    ingress.1.from_port:         "" =&gt; "-1"
    ingress.1.protocol:          "" =&gt; "icmp"
    ingress.1.security_groups.#: "" =&gt; "0"
    ingress.1.self:              "" =&gt; "1"
    ingress.1.to_port:           "" =&gt; "-1"
    ingress.2.cidr_blocks.#:     "" =&gt; "0"
    ingress.2.from_port:         "" =&gt; "0"
    ingress.2.protocol:          "" =&gt; "tcp"
    ingress.2.security_groups.#: "" =&gt; "0"
    ingress.2.self:              "" =&gt; "1"
    ingress.2.to_port:           "" =&gt; "65535"
    name:                        "" =&gt; "private"
    owner_id:                    "" =&gt; "&lt;computed&gt;"
    vpc_id:                      "" =&gt; "${var.vpc_id}"

+ module.mesos.aws_security_group.slave
    description:                 "" =&gt; "slave"
    ingress.#:                   "" =&gt; "1"
    ingress.0.cidr_blocks.#:     "" =&gt; "1"
    ingress.0.cidr_blocks.0:     "" =&gt; "0.0.0.0/0"
    ingress.0.from_port:         "" =&gt; "5051"
    ingress.0.protocol:          "" =&gt; "tcp"
    ingress.0.security_groups.#: "" =&gt; "0"
    ingress.0.self:              "" =&gt; "0"
    ingress.0.to_port:           "" =&gt; "5051"
    name:                        "" =&gt; "slave"
    owner_id:                    "" =&gt; "&lt;computed&gt;"
    vpc_id:                      "" =&gt; "${var.vpc_id}"

+ module.mesos.aws_subnet.public
    availability_zone:       "" =&gt; "us-east-1e"
    cidr_block:              "" =&gt; "10.0.1.0/24"
    map_public_ip_on_launch: "" =&gt; "1"
    vpc_id:                  "" =&gt; "${var.vpc_id}"

+ module.mesos.null_resource.init_ceph
</code></pre>

<p>Now, you can do <code>terraform apply</code> and look into instances or Mesos/Marathon UI.</p>

<h2>Inside Terraform module</h2>

<p>There are a few techniques to avoid some difficulties in cluster management and more.</p>

<h3>Ceph cluster initialization process</h3>

<p>To start using Ceph cluster and Ceph FS, you have to run some procedures like this:</p>

<ul>
<li>Create <code>ceph.conf</code> for the cluster</li>
<li>Deploy <code>ceph.conf</code> to <code>mon</code> servers, initialize each <code>mon</code> and gather keys generated on each servers</li>
<li>Deploy keys to <code>osd</code> servers and initialize each <code>osd</code></li>
<li>Deploy keys to <code>mds</code> servers and initialize each <code>mds</code></li>
<li>Create data and metadata OSD pool</li>
<li>Create Ceph FS using these pools</li>
<li>Mount Ceph FS pointing <code>mon</code> servers</li>
</ul>


<p>Since they are too complicated and stateful, there is a big problem to manage Ceph cluster by Terraform or others.</p>

<p><code>ceph-deploy</code> is a easy tool for these procedure, so I used it on the admin instance. But how can we run a initial procedure by Terraform?</p>

<p><code>null_resource</code> is the best way. See <a href="https://github.com/riywo/mesos-ceph/blob/17807b073803d92430adcba28b5e784615de2f02/terraform/init_ceph.tf">terraform/init_ceph.tf</a></p>

<p>After spinning up all instances, this resource runs provisioners to initialize Ceph cluster like above. Once created this virtual resource, this procedure won&rsquo;t be executed any more.</p>

<h3>Ceph initializing and provisioning</h3>

<p>Considering initializing process above, there are two types of resource creation; Initializing and provisioning.</p>

<p>Initializing means the completely initial timing. At this time, each instance doesn&rsquo;t need to be provisioned because the cluster initialization process will do everything.</p>

<p>Provisioning, on the other hand, means after the cluster is initialized. For example, when a master/slave/admin instance is terminated, Terraform tries to re-create the instance and this is provisioning. It is a little bit different process than initializing because the cluster already exists.</p>

<p>See <a href="https://github.com/riywo/mesos-ceph/blob/17807b073803d92430adcba28b5e784615de2f02/terraform/scripts/init_master.sh">terraform/scripts/init_master.sh</a>. <code>if ceph_initialized;</code> block is for this problem. So, all instances can be terminated anytime even after a Ceph cluster is initialized. Try terminate a instance and <code>terraform apply</code>!</p>

<h3>SSH gateway and provisioners</h3>

<p>Terraform provisioners assume the instance can be connected by ssh directly. In this module, I don&rsquo;t open ssh port for cluster instances except admin instance, so it is a SSH gateway.</p>

<p>If you configure <code>connection</code> block to the gateway instead of the actual instance, you can use provisioners, but they run on the gateway.</p>

<p>So, I copy AWS key file into the admin instance to be able to login to other instances (there is not a way to use agent forward so far).</p>

<p>For each instance provisioning, I upload script files prefixed by the instance id to prevent overwriting race condition. See <a href="https://github.com/riywo/mesos-ceph/blob/17807b073803d92430adcba28b5e784615de2f02/terraform/master1.tf">terraform/master1.tf</a>. <code>script_path</code> option for <code>connection</code> is undocumented so far, btw.</p>

<h3>Master IP addresses</h3>

<p>I want to fix master IP addresses because they are hardcoded anywhere. I tried Terraform variable as list of IP addresses, but so far it was impossible.</p>

<p>Because of that, I fixed the number of masters and created each <code>aws_instance</code> resource.</p>

<p>When the list variable feature is implemented, I will refactor these configuration.</p>

<h3>Concatenate provisioner scripts</h3>

<p>I use bash script for provisioners and there are a lot of common functions, so I wrote a shared script <a href="https://github.com/riywo/mesos-ceph/blob/17807b073803d92430adcba28b5e784615de2f02/terraform/scripts/header.sh">terraform/scripts/header.sh</a>.</p>

<p>To run each provisioner correctly, this file must be concatenated. Also, an entry point <code>main</code> must be concatenated. So I did like this:</p>

<pre><code>provisioner "remote-exec" {
        inline = [
            "echo main foo bar | cat header.sh init_foo.sh - | bash"
        ]
}
</code></pre>

<p>I think there are much better ways, though&hellip;</p>

<h2>Isolation between Terraform and Packer</h2>

<p>There is a philosophy:</p>

<ul>
<li>Packer

<ul>
<li>Install file resources from the Internet</li>
</ul>
</li>
<li>Terraform

<ul>
<li>Install only runtime information, like IP address

<ul>
<li>Do not fetch file resources from the Internet</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>So, any <code>apt-get install</code> are done by Packer nor Terraform. This philosophy is like <a href="http://12factor.net/">The Twelve-Factor App</a>.</p>

<h2>BTW, <code>awk</code> is great</h2>

<p>I wanted a way to use the instance id built by Packer in Terraform automatically. I found that Packer has <code>-machine-readable</code> output option and it is CSV format. So, I started to write a processor script:</p>

<ul>
<li>Print the machine readable output as well as normal Packer output</li>
<li>Write <code>ami.tf</code> file using the instance id built just now</li>
</ul>


<p>See <a href="https://github.com/riywo/mesos-ceph/blob/17807b073803d92430adcba28b5e784615de2f02/packer/process">packer/process</a>. This is my first <code>awk</code> executable script. <code>awk</code> was very nice in this case and I could find a lot of <code>awk</code> tips.</p>

<h2>Understanding basystyle</h2>

<p><a href="https://github.com/progrium/bashstyle">progrium/bashstyle</a></p>

<p>To write complex shell script for Packer/Terraform, I read the bashstyle article above. I don&rsquo;t understand all of them, but there are tons of best practice.</p>

<h1>Conclusion</h1>

<p>It was fun!</p>

<p>Hey, wait a moment&hellip; At the beginning, I just wanted to run many applications on Mesos using Docker, for example WordPress, MySQL Galera Cluster, etc. To implement them, I needed a shared file system, so I started to learn about Ceph which I had an eye on before. To deploy the cluster into AWS, I needed some orchestration software, then started to see Terraform, Packer to create AMI&hellip; Too long yak shaving it was ;(</p>

<p>Now, let&rsquo;s start learning about Mesos/Marathon/Docker and many applications!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fluentd on Mesos + Docker + Marathon]]></title>
    <link href="http://tech.riywo.com/blog/2013/12/20/fluentd-on-mesos-plus-docker-plus-marathon/"/>
    <updated>2013-12-20T17:13:00+09:00</updated>
    <id>http://tech.riywo.com/blog/2013/12/20/fluentd-on-mesos-plus-docker-plus-marathon</id>
    <content type="html"><![CDATA[<p>I just tried to run Fluentd instances on Mesos + Docker + Marathon in Vagrant boxes. Here is the sample repository.</p>

<ul>
<li><a href="https://github.com/riywo/sample-fluentd-on-mesos-docker">riywo/sample-fluentd-on-mesos-docker</a></li>
</ul>


<p>The overview diagram:</p>

<p><img src="https://cacoo.com/diagrams/9XIEXwoUNUvOOuZ2-AB7C1.png" alt="diagram" /></p>

<!-- more-->


<h2>How?</h2>

<p>See the github README.md.</p>

<h2>What?</h2>

<h3>Fluentd</h3>

<ul>
<li><a href="http://fluentd.org/">Fluentd: Open Source Log Management</a></li>
</ul>


<p>Fluentd is one of log collector middleware. Most web companies in Japan use this for log collecting. td-agent is a package version of fluentd including ruby interpreter.</p>

<p>Some uses want to spin up multiple fluentd workers across many physical servers because fluentd only use one cpu core. Mesos is the right software to manage such situation.</p>

<h3>Mesos</h3>

<ul>
<li><a href="http://mesos.apache.org/">Apache Mesos</a></li>
</ul>


<p>Computer resource management software. You can combine many servers as a one &ldquo;virtual computer&rdquo; with Mesos. Mesos itself is just allocating resources, so you have to use frameworks to manage these resources.</p>

<h3>Marathon</h3>

<ul>
<li><a href="https://github.com/mesosphere/marathon">mesosphere/marathon</a></li>
</ul>


<p>Marathon is a framework on Mesos, which is like &ldquo;upstart for virtual computer&rdquo;. Marathon makes sure the number of running tasks, so if one of Mesos slave goes down, Marathon starts new instances on other slaves.</p>

<p>Marathon also manages port of instances. You can get the list of host:port of your applications just asking Marathon API; this is kind of service discovery.</p>

<h3>Docker</h3>

<ul>
<li><a href="https://www.docker.io/">Homepage - Docker: the Linux container engine</a></li>
</ul>


<p>LXC manager. You don&rsquo;t have to take care of server environment any more. Just create a docker container and commit it. Then, you can run the application anywhere you want. This is very convenient for computer cluster like Mesos.</p>

<p>Mesosphere provides a simple Mesos executor for docker. It runs docker container and bind ports, so you can connect containers from outside of the slave.</p>

<h3>HAProxy</h3>

<ul>
<li><a href="http://haproxy.1wt.eu/">HAProxy - The Reliable, High Performance TCP/HTTP Load Balancer</a></li>
</ul>


<p>Fluentd has http input interface, so I connect them with HAProxy running on Docker. First getting the list of host:port from Marathon, then generating config file of HAProxy. If the list is updated, just restart the HAProxy container.</p>

<h3>Elasticsearch, Kibana</h3>

<ul>
<li><a href="http://www.elasticsearch.org/">Open Source Distributed Real Time Search &amp; Analytics | Elasticsearch</a></li>
<li><a href="http://www.elasticsearch.org/overview/kibana/">Kibana | Overview | Elasticsearch</a></li>
</ul>


<p>Elasticsearch and Kibana is one of the best combination of storing logs and visualizing them.</p>

<p>Fluentd has pluggable input/output and there is Elasticsearch output plugin. I installed it in the fluentd docker image. Giving ES host:port to the container as environment variables, fluentd can connect the ES. I run ES and Kibana on Docker, too.</p>

<h2>Conclusion</h2>

<p>It was very interesting exercise for me. I&rsquo;d like to keep practicing Mesos, Docker, Marathon, Vagrant, whatever. I hope this sample helps you, too.</p>

<p>BTW, there were so much trouble especially Vagrant integration. <code>/etc/hosts</code> file is very tricky. I&rsquo;d like to know more reasonable way to manage Vagrant hosts information…</p>

<h2>References</h2>

<ul>
<li><a href="https://github.com/mesosphere/mesos-docker">mesosphere/mesos-docker</a></li>
<li><a href="http://mesosphere.io/2013/09/26/docker-on-mesos/">Mesosphere · Docker on Mesos</a></li>
<li><a href="http://techcrunch.com/2013/09/26/mesosphere-adds-docker-support-to-its-mesos-based-operating-system-for-the-data-center/">Mesosphere Adds Docker Support To Its Mesos-Based Operating System For The Data Center | TechCrunch</a></li>
<li><a href="http://www.slideshare.net/AishFenton/mesos-docker">Mesos ♥ Docker</a>

<ul>
<li><a href="http://www.youtube.com/watch?v=2MmnggSmTJo">▶ Docker ♥ Mesos - YouTube</a></li>
</ul>
</li>
<li><a href="http://typesafe.com/blog/play-framework-grid-deployment-with-mesos">Play Framework Grid Deployment with Mesos – Blog - Typesafe</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[mesos tutorial 3]]></title>
    <link href="http://tech.riywo.com/blog/2013/09/29/mesos-tutorial-3/"/>
    <updated>2013-09-29T10:24:00+09:00</updated>
    <id>http://tech.riywo.com/blog/2013/09/29/mesos-tutorial-3</id>
    <content type="html"><![CDATA[<p>To understand development of Mesos Framework, I developed a simple <strong>useless</strong> framework example called <code>msh</code>.</p>

<ul>
<li><a href="https://github.com/riywo/msh">riywo/msh</a></li>
</ul>


<!-- more -->


<h2>Usage</h2>

<p>At first, you should distribute this directory to the all slaves on the same location.</p>

<p>Then, you can run <code>msh</code> command on any server. For example:</p>

<pre><code>$ cat /etc/hosts | ./msh -- cat -n 2&gt;/dev/null
     1  127.0.0.1 localhost
     2
     3  # The following lines are desirable for IPv6 capable hosts
     4  ::1 ip6-localhost ip6-loopback
     5  fe00::0 ip6-localnet
     6  ff00::0 ip6-mcastprefix
     7  ff02::1 ip6-allnodes
     8  ff02::2 ip6-allrouters
     9  ff02::3 ip6-allhosts
</code></pre>

<h2>Internal</h2>

<ul>
<li><code>msh</code> launches a <code>msh-executor</code> using one of resource offers</li>
<li><code>msh-executor</code> starts the given command as a subprocess</li>
<li><code>msh</code> reads its stdin and send the input to the slave as FrameworkMessage</li>
<li><code>msh-executor</code> receives messages and write them to the subprocess.stdin</li>
<li><code>msh-executor</code> reads the subprocess.stdout and sends the output to <code>msh</code> as FrameworkMessage</li>
<li>Each other sends a message &lsquo;EOF&rsquo; as a signal of end of the pipe</li>
<li><code>msh</code> prints received message</li>
</ul>


<h2>TODO</h2>

<ul>
<li>work with no stdin</li>
<li>streaming pipe</li>
<li>pipe multiple tasks like UNIX shell</li>
<li>interactive shell</li>
<li>error handling</li>
</ul>


<h2>Conclusion</h2>

<p>Development of Mesos framework in Python is very easy. I hope this toy project is useful for someone who is interested in Mesos.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[mesos tutorial 2]]></title>
    <link href="http://tech.riywo.com/blog/2013/09/28/mesos-tutorial-2/"/>
    <updated>2013-09-28T00:02:00+09:00</updated>
    <id>http://tech.riywo.com/blog/2013/09/28/mesos-tutorial-2</id>
    <content type="html"><![CDATA[<p>In the pervious post, I tried &ldquo;cpu = 0.1&rdquo; for Mesos tasks. But it didn&rsquo;t work precisely. If you have only one cpu on each slave server, you can run only one task on each slave at the same time even if you specify &ldquo;cpu = 0.1&rdquo; for the tasks.</p>

<p>It only happens on the slave which has just one cpu. If a slave has more than one cpus, it can run tasks more than the number of cpus. I use m1.small and it has only one cpu, unfortunately…</p>

<p>So, I changed Mesosphere&rsquo;s runner script a little to configure <code>--resources</code> flag of <code>mesos-slave</code>.</p>

<pre><code>/usr/bin/mesos-init-wrapper
@@ -22,6 +22,7 @@
   [[ ! ${ULIMIT:-} ]] || ulimit $ULIMIT
   [[ ! ${MASTER:-} ]] || args+=( --master="$MASTER" )
   [[ ! ${LOGS:-} ]]   || args+=( --log_dir="$LOGS" )
+  [[ ! ${RESOURCES:-} ]] || args+=( --resources="$RESOURCES" )
   logged /usr/local/sbin/mesos-slave "${args[@]}"
 }
</code></pre>

<p>And set more cpus.</p>

<pre><code>$ cat /etc/default/mesos-slave
MASTER=`cat /etc/mesos/zk`
RESOURCES="cpus:2"
</code></pre>

<p>Now you can see 2 cpus in your Mesos cluster, then you can run more than 2 tasks on it at the same time.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[mesos introduction 1]]></title>
    <link href="http://tech.riywo.com/blog/2013/09/27/mesos-introduction-1/"/>
    <updated>2013-09-27T00:53:00+09:00</updated>
    <id>http://tech.riywo.com/blog/2013/09/27/mesos-introduction-1</id>
    <content type="html"><![CDATA[<p>Today, Mesosphere released an important milestone. I really desired this integration.</p>

<ul>
<li><a href="http://mesosphere.io/2013/09/26/docker-on-mesos/">Mesosphere · Docker on Mesos</a></li>
</ul>


<p>Now, we can manage our &ldquo;Computers as an OS&rdquo;.</p>

<ul>
<li>Kernel

<ul>
<li><a href="http://mesos.apache.org/">Mesos</a></li>
</ul>
</li>
<li>Process

<ul>
<li><a href="https://www.docker.io/">Docker</a></li>
</ul>
</li>
<li>Init

<ul>
<li><a href="https://github.com/mesosphere/marathon">Marathon</a></li>
</ul>
</li>
<li>Cron

<ul>
<li><a href="https://github.com/airbnb/chronos">Chronos</a></li>
</ul>
</li>
</ul>


<!-- more -->


<p>I would like to learn this new generation &ldquo;OS&rdquo;, so I started to play with Mesos. Mesosphere is a great company and they built packages of Mesos and Marathon. Building Mesos and tools are most painful part of this ecosystem, but now we can super easily start to use Mesos and Marathon. Thanks, Mesosphere:)</p>

<p>I use AWS for Mesos cluster. At first, I created a m1.small instance for a master of Mesos, Zookeeper and Marathon. I use EC2-Classic.</p>

<ul>
<li>AMI

<ul>
<li>Ubuntu Server 13.04</li>
</ul>
</li>
<li>Security Group

<ul>
<li>From any src IP

<ul>
<li>22(ssh)</li>
<li>2181(zk)</li>
<li>5050(mesos master)</li>
<li>5051(mesos slave)</li>
<li>8080(marathon)</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>And I set user-data as an initial setup script:</p>

<pre><code>#!/bin/bash
curl -fL https://raw.github.com/mesosphere/mesos-docker/master/bin/mesos-docker-setup | bash
echo manual &gt;&gt; /etc/init/mesos-slave.conf
reboot
</code></pre>

<p>After the instance is ready, you can access Mesos(5050) and Marathon(8080) from your browser. I associated an EIP to the master.</p>

<p>Then, I requested 10 spot instances for slaves. Same type(m1.small), same AMI, same security group, and user-data:</p>

<pre><code>#!/bin/bash
curl -fL https://raw.github.com/mesosphere/mesos-docker/master/bin/mesos-docker-setup | bash
echo manual &gt;&gt; /etc/init/mesos-master.conf
echo manual &gt;&gt; /etc/init/marathon.conf
echo manual &gt;&gt; /etc/init/zookeeper.conf
echo 'zk://YOUR_MASTER_EIP:2181/mesos' &gt; /etc/mesos/zk
reboot
</code></pre>

<p>After a while, you can see 10 slaves, 10 CPUs and 6GB Mem on your Mesos dashboard! Wow! This is your new &ldquo;OS&rdquo;!</p>

<p>Now you can run ANY process using Mesosphere&rsquo;s Mesos, Marathon and Docker integration. Why don&rsquo;t you request Redis processes to your Marathon?</p>

<pre><code>$ http POST http://YOUR_MASTER_EIP:8080/v1/apps/start \
          id=multidis instances=2 mem=512 cpus=0.5 \
          executor=/var/lib/mesos/executors/docker \
          cmd='johncosta/redis'
</code></pre>

<p>This request will start 2 tasks and use 1 CPU and 1GB Mem. You can scale it very easily. Awesome…, awesome…</p>

<h2>Next step</h2>

<ul>
<li>Build Chronos</li>
<li>Run Chronos on Marathon</li>
<li>Create Docker image and run it on Marathon</li>
<li>Integration with <a href="https://github.com/coreos/etcd">etcd</a>, or something

<ul>
<li>Configuration of 12factor app</li>
</ul>
</li>
<li>Integration with <a href="http://ceph.com/">Ceph</a> or other storage

<ul>
<li>For virtual storage</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
</feed>
